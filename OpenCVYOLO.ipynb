{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cam_util\n",
    "\n",
    "from models import *\n",
    "import utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "GPU #0: GeForce GTX 1050 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Checks GPU availability\n",
    "assert(torch.cuda.is_available())\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs: {}\".format(num_gpus))\n",
    "for i in range(num_gpus):\n",
    "    print(\"GPU #{}: {}\".format(i, torch.cuda.get_device_name(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Defaults to CPU if GPU is not available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV variables\n",
    "# Choose camera\n",
    "cam_id = 0\n",
    "# Speech variables\n",
    "with_speech = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 camera(s)\n"
     ]
    }
   ],
   "source": [
    "cam_array = cam_util.get_camera_array(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO paths and configuration variables\n",
    "model_def = \"config/yolov3.cfg\"\n",
    "weights_path = \"weights/yolov3.weights\"\n",
    "class_path = \"data/coco.names\"\n",
    "# img size on which to run inference\n",
    "img_size = 416 # should be 416 as this is the size YOLO was trained on\n",
    "# confidence threshold for class consideration\n",
    "conf_thres = 0.9\n",
    "nms_thres = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane' 'apple' 'backpack' 'banana' 'baseball bat' 'baseball glove'\n",
      " 'bear' 'bed' 'bench' 'bicycle' 'bird' 'boat' 'book' 'bottle' 'bowl'\n",
      " 'broccoli' 'bus' 'cake' 'car' 'carrot' 'cat' 'cell phone' 'chair' 'clock'\n",
      " 'cow' 'cup' 'diningtable' 'dog' 'donut' 'elephant' 'fire hydrant' 'fork'\n",
      " 'frisbee' 'giraffe' 'hair drier' 'handbag' 'horse' 'hot dog' 'keyboard'\n",
      " 'kite' 'knife' 'laptop' 'microwave' 'motorbike' 'mouse' 'orange' 'oven'\n",
      " 'parking meter' 'person' 'pizza' 'pottedplant' 'refrigerator' 'remote'\n",
      " 'sandwich' 'scissors' 'sheep' 'sink' 'skateboard' 'skis' 'snowboard'\n",
      " 'sofa' 'spoon' 'sports ball' 'stop sign' 'suitcase' 'surfboard'\n",
      " 'teddy bear' 'tennis racket' 'tie' 'toaster' 'toilet' 'toothbrush'\n",
      " 'traffic light' 'train' 'truck' 'tvmonitor' 'umbrella' 'vase'\n",
      " 'wine glass' 'zebra']\n"
     ]
    }
   ],
   "source": [
    "# Get COCO classes\n",
    "classes = utils.load_classes(class_path) \n",
    "print(np.sort(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: weights/yolov3.weights\n"
     ]
    }
   ],
   "source": [
    "# Load model from file\n",
    "# Initiate model\n",
    "model = Darknet(model_def).to(device)\n",
    "if weights_path.endswith(\".weights\"):\n",
    "    # Load darknet weights\n",
    "    model.load_darknet_weights(weights_path)\n",
    "else:\n",
    "    # Load checkpoint weights\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "# Put model on desired device\n",
    "model = model.to(device)\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded from: {}\".format(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_YOLO_on_frame(frame, model, conf_thres=0.9, nms_thres=0.5, colors=None):\n",
    "    detections = None\n",
    "    unique_labels = None\n",
    "    bbox_colors = None\n",
    "    if colors is None:\n",
    "        colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "    # Convert webcam frame to Torch batch\n",
    "    img_batch = cam_util.webcam_frame_to_torch_batch(frame, BGR_TO_RGB=True, pad_to_square=True, img_size=img_size)\n",
    "    # Throw image on chosen device\n",
    "    img_batch = img_batch.to(device)\n",
    "    # Perform inference on network\n",
    "    outputs = model(img_batch)\n",
    "    # Eliminate low confidence detections\n",
    "    detections = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n",
    "    # Detections is a list, but we know it only contains detections for one input image\n",
    "    detections = detections[0]\n",
    "    if detections is not None:\n",
    "        # Rescale bounding boxes to current frame img size\n",
    "        detections = utils.rescale_boxes(detections, img_size, frame_webcam.shape[:2])\n",
    "        # Get uniquely predicted labels\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        # Get number of predicted classes\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        # Get bounding boxes' colors\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    return detections, unique_labels, bbox_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_YOLO_detections_on_frame(frame, detections, bbox_colors, unique_labels):\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections: \n",
    "        cls_str = classes[int(cls_pred)]\n",
    "        # Get bbox coordinates, width and height\n",
    "        left = int(x1.item())  \n",
    "        bottom = int(y1.item())\n",
    "        right = int(x2.item() )\n",
    "        top = int(y2.item()) \n",
    "        box_w = right - left\n",
    "        box_h = top - bottom\n",
    "        # Get bbox color\n",
    "        bbox_label = \"{} {:.2f}\".format(cls_str, cls_conf.item())\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        cv2.putText(frame, bbox_label, (left, bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255))\n",
    "        cv2.rectangle(frame, (left, bottom), (right, top), (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-183ed5e9784f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0myolo_frame_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get a reference to a video capturer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvideo_capture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcam_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcam_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#video_capture2 = cv2.VideoCapture(cam_array[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# loop until ESC key is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Possible bounding-box colors\n",
    "cmap = plt.get_cmap(\"tab20b\")\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "# Interval on which to run YOLO inference\n",
    "yolo_frame_interval = 10\n",
    "# Get a reference to a video capturer\n",
    "video_capture = cv2.VideoCapture(cam_array[cam_id])\n",
    "#video_capture2 = cv2.VideoCapture(cam_array[1])\n",
    "# loop until ESC key is pressed\n",
    "frame_idx = 0\n",
    "detections = None\n",
    "detections2 = None\n",
    "while not cv2.waitKey(33) == 27:\n",
    "    frame_idx += 1\n",
    "    if frame_idx > 1000:\n",
    "        frame_idx = 1\n",
    "    # Grab a single frame of video\n",
    "    _, frame_webcam = video_capture.read()\n",
    "    #_, frame_webcam2 = video_capture2.read()\n",
    "    # predict for current downsized frame    \n",
    "    start = time.time()\n",
    "    # Only run inference if we are at the right interval\n",
    "    if frame_idx % yolo_frame_interval == 0:\n",
    "        detections, unique_labels, bbox_colors = run_YOLO_on_frame(frame_webcam, model, colors=colors, conf_thres=conf_thres)\n",
    "        #detections2, unique_labels2, bbox_colors2 = run_YOLO_on_frame(frame_webcam2, model, colors=colors)\n",
    "    # Calculate elapsed time (also in FPS)\n",
    "    end = time.time()\n",
    "    elapsed_sec = end - start        \n",
    "    if elapsed_sec < 0.0417:\n",
    "        fps = 'MAX'\n",
    "    else:\n",
    "        fps = str(int(1/elapsed_sec))\n",
    "    # Display frame inference time\n",
    "    time_lapse_label = 'Inference time: {:.0f} ms or {} fps'.format(elapsed_sec*1000, fps)\n",
    "    cv2.putText(frame_webcam, time_lapse_label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "    # Display latest inferred detections\n",
    "    if detections is not None:\n",
    "        show_YOLO_detections_on_frame(frame_webcam, detections, bbox_colors, unique_labels)\n",
    "    #if detections2 is not None:\n",
    "    #    show_YOLO_detections_on_frame(frame_webcam2, detections2, bbox_colors2, unique_labels2, speech_engine=speech_engine)\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('OPenCV YOLO 1', frame_webcam)\n",
    "    #cv2.imshow('OPenCV YOLO 2', frame_webcam2)\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
